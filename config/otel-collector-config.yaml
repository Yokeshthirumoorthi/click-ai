# ============================================================
#  OpenTelemetry Collector Configuration
#
#  Flow:
#    Your App
#      → sends OTLP events to port 4317 (gRPC) or 4318 (HTTP)
#      → Collector receives them in the "receivers" section
#      → processes/batches them in the "processors" section
#      → writes to ClickHouse in the "exporters" section
# ============================================================

receivers:
  # ── OTLP receiver ──────────────────────────────────────────
  # This is the standard way any app sends telemetry data.
  # Supports TRACES, METRICS, and LOGS all on the same port.
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317 # your app connects here
      http:
        endpoint: 0.0.0.0:4318 # or here if using HTTP

processors:
  # ── Batch processor ────────────────────────────────────────
  # Instead of writing every single event one by one (slow!),
  # the collector waits and groups them into batches.
  # 5000 events OR 5 seconds — whichever comes first → flush.
  batch:
    send_batch_size: 5000 # flush after 5000 events
    timeout: 5s # or after 5 seconds
    send_batch_max_size: 10000 # never exceed 10000 per batch

  # ── Memory limiter ─────────────────────────────────────────
  # Safety valve: if the collector is using too much RAM,
  # it starts dropping events rather than crashing.
  memory_limiter:
    check_interval: 1s
    limit_mib: 2048 # 2GB max for the collector process
    spike_limit_mib: 512

exporters:
  # ── ClickHouse exporter ────────────────────────────────────
  # This writes your telemetry data into ClickHouse tables.
  # The exporter auto-creates the tables if they don't exist!
  clickhouse:
    endpoint: tcp://clickhouse:9000 # ClickHouse native port
    username: admin
    password: clickhouse123
    database: otel # all tables go into this DB

    # TTL: auto-delete old data to save disk space
    # "72h" = delete data older than 3 days
    ttl: 72h

    # How many rows to buffer before writing
    # Larger = faster inserts, more memory used
    compress: lz4

    # Table names (these are created automatically)
    logs_table_name: otel_logs
    traces_table_name: otel_traces
    metrics_table_name: otel_metrics

    # Retry on failure — don't lose data if ClickHouse blips
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 300s

    # Queue events in memory if ClickHouse is temporarily slow
    sending_queue:
      enabled: true
      num_consumers: 10 # parallel write workers
      queue_size: 10000 # max events buffered in memory

  # ── Debug logger (optional) ────────────────────────────────
  # Prints events to the collector's stdout so you can verify
  # data is flowing. Remove or set to "none" in production.
  debug:
    verbosity: basic # options: basic | normal | detailed

extensions:
  # Health check endpoint — used by Docker healthcheck
  health_check:
    endpoint: 0.0.0.0:13133

  # zpages: browser-based debug UI at http://localhost:55679
  # Shows live traces, pipeline stats, etc.
  zpages:
    endpoint: 0.0.0.0:55679

# ── Pipeline ───────────────────────────────────────────────────────
# Wire everything together.
# Each pipeline = one type of telemetry data (logs / traces / metrics)
service:
  extensions: [health_check, zpages]

  pipelines:
    # LOGS pipeline: app logs → ClickHouse otel_logs table
    logs:
      receivers: [otlp]
      processors: [memory_limiter, batch]
      exporters: [clickhouse, debug]

    # TRACES pipeline: request traces → ClickHouse otel_traces table
    traces:
      receivers: [otlp]
      processors: [memory_limiter, batch]
      exporters: [clickhouse, debug]

    # METRICS pipeline: counters/gauges → ClickHouse otel_metrics table
    metrics:
      receivers: [otlp]
      processors: [memory_limiter, batch]
      exporters: [clickhouse, debug]
